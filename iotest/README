Introduction
------------
Iotest is a program to test performance of parallel I/O of large arrays. Assume a given large three dimensional array, of size n1 x n2 x n3 floating point numbers in double precision. The array is stored on disk as a single file, as a stream of bytes. Iotest distributes the array onto all its MPI-tasks, reads the array into the processors, and outputs the time it took to read it.

The program iotest has three different sets of I/O routines, a) the parallel I/O implemented in SW4 and ADPDIS3D, b) parallel HDF5, and c) an older set of parallel I/O routines that does not rely on message passing. The purpose is to compare the performance and test the correctness of these routines.

There are two executables: 

gentestfile - For generation of a large file, runs on a single processor.

iotest      - An MPI-parallel program that reads the file generated by gentestfile and outputs the time it took to read it. Iotest also verifies that the file is read correctly.

To build
--------

To build gentestfile, cd to the iotest directory and type
make gentestfile

To build iotest, cd to the iotest directory and type
make 

gentestfile and iotest can be built with or without parallel HDF5. HDF5 is used by default. You will have to edit the makefile to define variable HDF5ROOT to indicate the directory where parallel HDF5 is installed on your machine.

Alternatively, iotest can be built without HDF5 support by typing:
make hdf5=no

To run
------

gentestfile can take a while to run if the file is big, since it does not use parallel I/O. The reason for not using parallel I/O is that the method used to generate the file should be completely different from the I/O routines used in iotest. 

Example: Generate a file, named `testfile-10G.bin', of size 10 Gbyte:

./gentestfile -size 10000 -file testfile-10G.bin

The size is given in megabytes. File names extension *.bin gives a simple binary file, consisting of 3 integers n1,n2,n3 giving the dimension of the array, followed by n1*n2*n3 double precision floating point numbers, stored in column order. I.e., when accessed as a three dimensional array, ar(i,j,k), the position of the (i,j,k) element in the file is computed as i+n1*j+n1*n2*k.

If instead the file name is given the extension *.hdf5, as in 

./gentestfile -size 10000 -file testfile-10G.hdf5

a file in HDF5 format is generated.  The name of the data set is hard coded to "testdata". The HDF5 data is stored in row order, as used in the C programming language, i.e., the (i,j,k) element is stored in position i*n2*n3+j*n3+k in the file.

To read a file, run iotest, for example:

mpirun -np 16 ./iotest -file testfile-10G.bin -bufsize 8000000

Runs iotest on 16 processors. Iotest reads testfile-10G.bin, verifies that the array is read correctly, and outputs the time it took to read. The following output is produced:

Running MPI version 3.0
Read file with dimensions 1523 x 1077 x 762
 -->  size = 9.99909 G bytes 
processor mesh has 4 x 2 x 2 processors
Result is correct, max error 0
Read time is 7.72159 seconds 


-----------------------------------------------------------------------
Example run with output from gentestfile:

bjorn@cab687 66) ./gentestfile -size 10000 -file test-10G.bin
Generating array of dimensions 1523 x 1077 x 762
Writing file of size 9.99909 G bytes 
 written to k= 100
 written to k= 200
 written to k= 300
 written to k= 400
 written to k= 500
 written to k= 600
 written to k= 700
 time writing is 21.513 seconds 

-----------------------------------------------------------------------
Help is obtained by:
./gentestfile -h 
Usage: 
gentestfile -nowrite -size mbytes -file filename -ndisp nfreq 
         -nowrite  --> Only estimate dimensions, do not write a file
         -size mbytes  --> mbytes = size of generated file in megabytes (default 1000 = 1G byte)
         -file filename --> Name of file to generate, use extension *.hdf5 for hdf5 format (default testfile.bin)
         -ndisp nfreq --> Print progress when generating file every nfreq:th grid plane (default 100)



./iotest -h
Usage: 
iotest -file filename -wproc wfreq -old -bufsize npts -procdist 3d -procdim 2 2 4 -printdim -save -newfile

       -file filename --> Name of file to read, use extension *.hdf5 for hdf5 format (default testfile.bin)
       -wproc wfreq   --> Perform IO from every wfreq:th processor only (SW4 IO only). Default is 1, meaning all processors will perform IO.
       -old           --> Use older IO routines, not MPI-parallel. 
       -bufsize npts  --> Size of internal buffer in number of elements (SW4 IO only).
       -procdist [3d|2d]   --> 3d will distribute the 3d array on a 3d processor grid. 
                               2d will use 2d processor grid to distribute the first two dimensions of the array
       -procdim 2 2 4 --> Use processor grid of size indicated. The three numbers must have product
                          equal to the total number of processors used. -procdim overrides -procdist
       -save          --> Save timing numbers on file timings.dat
       -newfile       --> The -save option will append to timings.dat. By giving -newfile iotest will truncate the file, starting new recording at the first line
       -printdim      --> Print out information about array dimensions from each processor, for debug purpose
